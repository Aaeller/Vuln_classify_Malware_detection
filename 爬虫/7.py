import requests
from lxml import etree
import openpyxl
import hashlib
import time
import datetime


def get__jsl_clearance_s(data):
    """
    通过加密对比得到正确cookie参数
    :param data: 参数
    :return: 返回正确cookie参数
    """
    chars = len(data['chars'])
    for i in range(chars):
        for j in range(chars):
            __jsl_clearance_s = data['bts'][0] + data['chars'][i] + data['chars'][j] + data['bts'][1]
            encrypt = None
            if data['ha'] == 'md5':
                encrypt = hashlib.md5()
            elif data['ha'] == 'sha1':
                encrypt = hashlib.sha1()
            elif data['ha'] == 'sha256':
                encrypt = hashlib.sha256()
            encrypt.update(__jsl_clearance_s.encode())
            result = encrypt.hexdigest()
            if result == data['ct']:
                return __jsl_clearance_s


def setCookie(url):
    global session
    session = requests.session()


class CVE(object):
    def __init__(self, type_id):
        self.type_id = str(type_id)
        self.host_url = "https://www.cvedetails.com"
        self.start_url = "https://www.cvedetails.com/vulnerability-list.php?" + self.type_id
        self.base_url = "https://www.cvedetails.com/vulnerability-list.php?vendor_id=0&product_id=0&version_id=0&page={}&hasexp=0&opdos=0&opec=0&opov=0&opcsrf=0&opgpriv=0&opsqli=0&opxss=0&{}&opmemc=0&ophttprs=0&opbyp=0&opfileinc=0&opginf=0&cvssscoremin=0&cvssscoremax=0&year=0&month=0&cweid=0&order=1&trc=4893&sha=69098b0b30799b9520bf468c7bc060a7f756abf9"
    #   https://www.cvedetails.com/vulnerability-list.php?vendor_id=0&product_id=0&version_id=0&page={}&hasexp=0&opdos=0&opec=0&opov=0&opcsrf=0&opgpriv=0&opsqli=0&opxss=0&opdirt=0&opmemc=0&ophttprs=0&opbyp=0&{}&opginf=0&cvssscoremin=0&cvssscoremax=0&year=0&month=0&cweid=0&order=1&trc=26094&sha=38745b427397c23f6ed92e0ed2d3e114da828672

    def get_max_page(self):
        """
        通过列表页第一页获取最大分页页码, 最多选取10000条记录
        Returns: [type] -- [min(最大分页页码, 500)，整数]
        """
        response = session.get(url=self.start_url)
        url=self.start_url
        print(url)
        content = response.text
        html = etree.HTML(content)
        max_page = html.xpath("//div[@class='paging']//a[last()]/text()")[0]
        print(max_page)
        if max_page:
            max_page = int(max_page)
            return max(max_page, 1)

    def get_list_page(self, max_page):
        """
        获取列表页html内容
        Arguments: max_page {[int]} -- [最大分页页码]
        """
        for page in range(1,max_page+1):
            print("正在爬取列表页第<%s>页" % str(page))
            url = self.base_url.format(page,self.type_id)  # base_url = "https://www.cnvd.org.cn/flaw/typeResult?typeId={}&max={}&offset={}"
            print(url)
            response = session.get(url=url)
            time.sleep(0.1)
            content = response.text
            yield content

    def parse_list_page(self, content):
        """
        获取列表页中的详情页href，返回href列表
        Arguments:
            content {[str]} -- [列表页html内容]
        Returns:
            [list] -- [详情页href列表]
        """
        html = etree.HTML(content)
        href_list = html.xpath("//tr[@class='srrowns']//td[@nowrap]//a/@href")
        return href_list

    def handle_str(self, td_list):
        """
        字符串去除空格\r\n\t等字符
        Arguments:
            td_list {[list]} -- [获取文本的列表]
        Returns:
            [type] -- [返回处理后的合并文本]
        """
        result = ''
        for td_str in td_list[1:]:
            result += td_str.strip()
        result = "".join(result.split())
        # 去除"危害级别"中的括号
        if result.endswith("()"):
            result = result[:-2]
        return result

    def parse_detail_page(self, content):
        """
        从详情页中提取信息
        Arguments:
            content {[str]} -- [详情页html内容]
        Returns:
            [dict] -- [提取信息字典]
        """
        html = etree.HTML(content)
        item = {}
        item['id'] = '' if len(html.xpath("//div[@id='contentdiv']//tr//td/h1/a/text()"))==0 else html.xpath("//div[@id='contentdiv']//tr//td/h1/a/text()")[0]
        item['descript'] = '' if len(html.xpath("//div[@class='cvedetailssummary']/text()")) == 0 else html.xpath("//div[@class='cvedetailssummary']/text()")[0]
        item['type'] = '' if len(html.xpath("//table[@id='cvssscorestable']//span[@class='vt_dirt']/text()"))==0 else html.xpath("//table[@id='cvssscorestable']//span[@class='vt_dirt']/text()")[0]
        print(item)
        return item

    def get_detail_info(self, href):
        """
        通过详情页href，获取详情页html内容
        Arguments:
            href {[str]} -- [详情页href]
        Returns:
            [调用详情页解析函数] -- [提取信息]
        """
        # host_url = "https://www.cnvd.org.cn" <a href="/flaw/show/CNVD-2021-02038" title="MiniCMS目录遍历漏洞（CNVD-2021-02038）">
        url = self.host_url + href
        print("正在爬取详情页<%s>" % url)
        respons = session.get(url=url)
        time.sleep(0.1)
        return self.parse_detail_page(respons.content.decode())


def vulnerability_xlsx(vlist, type):
    workbook = openpyxl.load_workbook('cve.xlsx')
    sheetnames = workbook.sheetnames
    worksheet = workbook[type]
    max_row = worksheet.max_row
    print(max_row)
    max_col = worksheet.max_column
    column_names = ['id', 'descript','type']
    # worksheet.append(column_names)

    for i in range(len(vlist)):
        value = []
        cell_datas = []
        for j in range(len(column_names)):
            value.append(vlist[i][column_names[j]])
        for x in range(1, max_row):
            cell_data = worksheet.cell(row=x,column=1).value
            cell_datas.append(cell_data)
        if value[0] in cell_datas:
            print(value[0])
            print("exist")
        else:
            worksheet.append(value)
    workbook.save('cve.xlsx')


def spider(type_id):
    cve = CVE(type_id)
    max_page = cve.get_max_page()
    print(max_page)
    content_generator = cve.get_list_page(max_page)
    for content in content_generator:
        href_list = cve.parse_list_page(content)
        for href in href_list:
            item = cve.get_detail_info(href)
            vlist = []
            vlist.append(item)
            print(vlist)
            vulnerability_xlsx(vlist, vtype[type_id])
            print("ok")


if __name__ == "__main__":
    setCookie('https://www.cvedetails.com/')
    vtype = {'opdirt=1': 'Directory traversal'}
    # ,28: '应用程序',29: 'WEB应用',30:'数据库',31: '网络设备（交换机、路由器等网络端设备）',32: '安全产品', 33: '智能设备（物联网终端设备）',38: '工业控制系统'}
    # workbook = xlsxwriter.Workbook('cve_data.xlsx')
    for type_id in vtype.keys():
        print('正在爬取<%s>漏洞类型' % vtype[type_id])
        begin = datetime.datetime.now()
        spider(type_id)
        time.sleep(0.1)
        end = datetime.datetime.now()
        total_time = end - begin
        print('爬取', vtype[type_id], '漏洞的时间: ', total_time, sep='')
    # workbook.close()
